{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c562046",
   "metadata": {},
   "source": [
    "# YOLO-based Segmentation of Recoater Streaking & Spatter\n",
    "\n",
    "This notebook uses a pre-trained YOLOv8 segmentation model to detect recoater streaks and spatter\n",
    "in each layer of a Laser Powder Bed Fusion build. We then compute per-layer anomaly areas for\n",
    "downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da52d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from src.utils.yolo_segmentation import (\n",
    "    batch_predict_and_compute_areas,\n",
    "    load_hdf5_slice,\n",
    "    load_hdf5_stack,\n",
    "    load_yolo_model,\n",
    "    visualize_detections,\n",
    ")\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ecb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH     = os.path.abspath(\"../data/2021-07-13 TCR Phase 1 Build 1.hdf5\")\n",
    "CAMERA_PATH   = \"slices/camera_data/visible/0\"\n",
    "IMG_TRAIN_DIR = os.path.abspath(\"../data/images/train\")\n",
    "LBL_TRAIN_DIR = os.path.abspath(\"../data/labels/train\")\n",
    "DATA_YAML     = os.path.abspath(\"data.yaml\")\n",
    "\n",
    "# Classes\n",
    "CLASS_MAP = {1: \"spatter\", 2: \"streak\"}\n",
    "PIXEL_SIZE_MM2 = 0.01  # adjust to your calibration\n",
    "\n",
    "# YOLO parameters\n",
    "WEIGHTS      = \"yolov8s-seg.pt\"\n",
    "EPOCHS       = 50\n",
    "BATCH_SIZE   = 8\n",
    "IMG_SIZE     = 640\n",
    "CONF_THRESH  = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637da317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /piml-in-metal-am/notebooks/data.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    \"train\": IMG_TRAIN_DIR,\n",
    "    \"val\":   IMG_TRAIN_DIR,    # replace with real val split if you have one\n",
    "    \"nc\":    len(CLASS_MAP),\n",
    "    \"names\": list(CLASS_MAP.values()),\n",
    "}\n",
    "with open(DATA_YAML, \"w\") as f:\n",
    "    yaml.dump(cfg, f)\n",
    "print(\"Wrote\", DATA_YAML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for YOLO Training\n",
    "\n",
    "Before training the YOLO model, you need to prepare your dataset. This involves exporting images and their corresponding segmentation masks (labels) from the HDF5 file into a format that YOLO can understand (typically PNG files for images and masks).\n",
    "\n",
    "The script `src/utils/export_yolo_training_data.py` has been created to handle this process.\n",
    "\n",
    "**Run the export script from your project root directory in the terminal:**\n",
    "```bash\n",
    "python src/utils/export_yolo_training_data.py\n",
    "```\n",
    "\n",
    "This script will populate the `data/images/train` and `data/labels/train` directories, which are referenced by the `data.yaml` file used by YOLO for training.\n",
    "\n",
    "**Make sure you have run this script successfully before proceeding to the training step below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "os.makedirs(IMG_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(LBL_TRAIN_DIR, exist_ok=True)\n",
    "print(f\"Ensured training image directory exists: {IMG_TRAIN_DIR}\")\n",
    "print(f\"Ensured training label directory exists: {LBL_TRAIN_DIR}\")\n",
    "\n",
    "# Export images and masks\n",
    "print(f\"Reading HDF5 data from: {DATA_PATH}\")\n",
    "exported_n_layers = 0\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"ERROR: HDF5 file not found at {DATA_PATH}. Please check the DATA_PATH variable.\")\n",
    "else:\n",
    "    with h5py.File(DATA_PATH, \"r\") as h5:\n",
    "        if CAMERA_PATH not in h5:\n",
    "            print(f\"ERROR: CAMERA_PATH '{CAMERA_PATH}' not found in HDF5 file.\")\n",
    "        elif \"slices/segmentation_results/8\" not in h5 or \"slices/segmentation_results/3\" not in h5:\n",
    "            print(f\"ERROR: Segmentation result paths for spatter (8) or streak (3) not found in HDF5 file.\")\n",
    "        else:\n",
    "            exported_n_layers = h5[CAMERA_PATH].shape[0]\n",
    "            print(f\"Exporting {exported_n_layers} layers as images and masks...\")\n",
    "            for layer in range(exported_n_layers):\n",
    "                img = load_hdf5_slice(DATA_PATH, layer, CAMERA_PATH)\n",
    "                imageio.imwrite(os.path.join(IMG_TRAIN_DIR, f\"{layer:05d}.png\"), img)\n",
    "                \n",
    "                label = np.zeros(img.shape[:2], dtype=np.uint8)  # Background = 0\n",
    "                # Spatter (HDF5 class 8) -> YOLO class 0 (pixel value 1 in mask)\n",
    "                sp_mask = h5[\"slices/segmentation_results/8\"][layer].astype(bool)\n",
    "                label[sp_mask] = 1\n",
    "                # Streak (HDF5 class 3) -> YOLO class 1 (pixel value 2 in mask)\n",
    "                st_mask = h5[\"slices/segmentation_results/3\"][layer].astype(bool)\n",
    "                label[st_mask] = 2\n",
    "                imageio.imwrite(os.path.join(LBL_TRAIN_DIR, f\"{layer:05d}.png\"), label)\n",
    "            print(f\"Export complete: {exported_n_layers} layers exported to {IMG_TRAIN_DIR} and {LBL_TRAIN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify export\n",
    "print(f\"Checking content of {IMG_TRAIN_DIR} (expected for training)...\")\n",
    "train_images_found = []\n",
    "if os.path.exists(IMG_TRAIN_DIR) and os.path.isdir(IMG_TRAIN_DIR):\n",
    "    train_images_found = os.listdir(IMG_TRAIN_DIR)\n",
    "    print(f\"Found {len(train_images_found)} files in {IMG_TRAIN_DIR}.\")\n",
    "    if len(train_images_found) > 0:\n",
    "        print(f\"First 5 images: {train_images_found[:5]}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {IMG_TRAIN_DIR} is empty. Training will likely fail.\")\n",
    "else:\n",
    "    print(f\"ERROR: {IMG_TRAIN_DIR} does not exist or is not a directory. Training will fail.\")\n",
    "\n",
    "print(f\"Checking content of {LBL_TRAIN_DIR} (expected for training)...\")\n",
    "train_labels_found = []\n",
    "if os.path.exists(LBL_TRAIN_DIR) and os.path.isdir(LBL_TRAIN_DIR):\n",
    "    train_labels_found = os.listdir(LBL_TRAIN_DIR)\n",
    "    print(f\"Found {len(train_labels_found)} files in {LBL_TRAIN_DIR}.\")\n",
    "    if len(train_labels_found) > 0:\n",
    "        print(f\"First 5 labels: {train_labels_found[:5]}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {LBL_TRAIN_DIR} is empty. Training will likely fail.\")\n",
    "else:\n",
    "    print(f\"ERROR: {LBL_TRAIN_DIR} does not exist or is not a directory. Training will fail.\")\n",
    "\n",
    "# Check if the number of images and labels match expected output from export cell\n",
    "if 'exported_n_layers' in locals() and exported_n_layers > 0:\n",
    "    if len(train_images_found) == exported_n_layers and len(train_labels_found) == exported_n_layers:\n",
    "        print(f\"Number of images ({len(train_images_found)}) and labels ({len(train_labels_found)}) matches expected number of layers ({exported_n_layers}).\")\n",
    "    else:\n",
    "        print(f\"WARNING: Mismatch after export! Expected: {exported_n_layers}, Found images: {len(train_images_found)}, Found labels: {len(train_labels_found)}\")\n",
    "elif 'exported_n_layers' in locals() and exported_n_layers == 0:\n",
    "    print(f\"NOTE: Export cell reported 0 layers exported. Check HDF5 content and paths if this is unexpected.\")\n",
    "else:\n",
    "    print(f\"WARNING: 'exported_n_layers' variable not found from export cell. Cannot confirm counts against expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54050d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.129 🚀 Python-3.12.10 torch-2.7.0+cu126 CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/piml-in-metal-am/notebooks/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo_spatter_streak2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/segment, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/yolo_spatter_streak2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset '/piml-in-metal-am/notebooks/data.yaml' error ❌ Dataset '/piml-in-metal-am/notebooks/data.yaml' images not found, missing path '/piml-in-metal-am/data/images/train'\nNote dataset download directory is '/piml-in-metal-am/notebooks/datasets'. You can update this in '/home/vscode/.config/Ultralytics/settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/trainer.py:583\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    579\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    580\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    581\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    582\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/data/utils.py:454\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    453\u001b[39m     m += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote dataset download directory is \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. You can update this in \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[32m    455\u001b[39m t = time.time()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset '/piml-in-metal-am/notebooks/data.yaml' images not found, missing path '/piml-in-metal-am/data/images/train'\nNote dataset download directory is '/piml-in-metal-am/notebooks/datasets'. You can update this in '/home/vscode/.config/Ultralytics/settings.json'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = YOLO(WEIGHTS)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_YAML\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns/segment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myolo_spatter_streak\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/model.py:787\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    785\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/models/yolo/segment/train.py:49\u001b[39m, in \u001b[36mSegmentationTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m     47\u001b[39m     overrides = {}\n\u001b[32m     48\u001b[39m overrides[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/trainer.py:139\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainset, \u001b[38;5;28mself\u001b[39m.testset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/trainer.py:587\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    585\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset '/piml-in-metal-am/notebooks/data.yaml' error ❌ Dataset '/piml-in-metal-am/notebooks/data.yaml' images not found, missing path '/piml-in-metal-am/data/images/train'\nNote dataset download directory is '/piml-in-metal-am/notebooks/datasets'. You can update this in '/home/vscode/.config/Ultralytics/settings.json'"
     ]
    }
   ],
   "source": [
    "# Ensure output directories exist and export images/labels for training\n",
    "print(f\"Target training image directory: {IMG_TRAIN_DIR}\")\n",
    "print(f\"Target training label directory: {LBL_TRAIN_DIR}\")\n",
    "os.makedirs(IMG_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(LBL_TRAIN_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Reading HDF5 data from: {DATA_PATH}\")\n",
    "exported_n_layers = 0\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"ERROR: HDF5 file not found at {DATA_PATH}. Please check the DATA_PATH variable and ensure the file exists.\")\n",
    "else:\n",
    "    try:\n",
    "        with h5py.File(DATA_PATH, \"r\") as h5:\n",
    "            if CAMERA_PATH not in h5:\n",
    "                print(f\"ERROR: CAMERA_PATH '{CAMERA_PATH}' not found in HDF5 file. Available top-level keys: {list(h5.keys())}\")\n",
    "            elif \"slices/segmentation_results/8\" not in h5 or \"slices/segmentation_results/3\" not in h5:\n",
    "                print(f\"ERROR: Segmentation result paths for spatter (HDF5 class 8) or streak (HDF5 class 3) not found in HDF5 file.\")\n",
    "                if \"slices/segmentation_results\" in h5:\n",
    "                    print(f\"Available segmentation classes: {list(h5[\"slices/segmentation_results\"].keys())}\")\n",
    "            else:\n",
    "                exported_n_layers = h5[CAMERA_PATH].shape[0]\n",
    "                print(f\"Exporting {exported_n_layers} layers as images and masks...\")\n",
    "                for layer in range(exported_n_layers):\n",
    "                    img = load_hdf5_slice(DATA_PATH, layer, CAMERA_PATH)\n",
    "                    imageio.imwrite(os.path.join(IMG_TRAIN_DIR, f\"{layer:05d}.png\"), img)\n",
    "                    \n",
    "                    label = np.zeros(img.shape[:2], dtype=np.uint8)  # Background = 0\n",
    "                    # Spatter (HDF5 class 8) -> YOLO class 0 (pixel value 1 in mask)\n",
    "                    sp_mask = h5[\"slices/segmentation_results/8\"][layer].astype(bool)\n",
    "                    label[sp_mask] = 1\n",
    "                    # Streak (HDF5 class 3) -> YOLO class 1 (pixel value 2 in mask)\n",
    "                    st_mask = h5[\"slices/segmentation_results/3\"][layer].astype(bool)\n",
    "                    label[st_mask] = 2\n",
    "                    imageio.imwrite(os.path.join(LBL_TRAIN_DIR, f\"{layer:05d}.png\"), label)\n",
    "                print(f\"Export complete: {exported_n_layers} layers exported to {IMG_TRAIN_DIR} and {LBL_TRAIN_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during HDF5 processing or file export: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ff9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = YOLO(\"runs/segment/yolo_spatter_streak/weights/best.pt\")\n",
    "# test on layer 0\n",
    "with h5py.File(DATA_PATH, \"r\") as h5:\n",
    "    img0 = load_hdf5_slice(DATA_PATH, 0, CAMERA_PATH)\n",
    "res0 = finetuned(img0, imgsz=IMG_SIZE, conf=CONF_THRESH)\n",
    "vis0 = visualize_detections(img0, res0)\n",
    "plt.imshow(vis0); plt.axis(\"off\"); plt.title(\"Layer 0 - Fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c280801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00186b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load full stack into memory (or chunk manually if too big)\n",
    "stack = load_hdf5_stack(DATA_PATH, CAMERA_PATH)  # shape (N,H,W,3)\n",
    "N = stack.shape[0]\n",
    "\n",
    "# 2) preallocate\n",
    "recoater_areas = np.zeros(N, dtype=float)\n",
    "spatter_areas  = np.zeros(N, dtype=float)\n",
    "\n",
    "# 3) run in batches\n",
    "for i in range(0, N, BATCH_SIZE):\n",
    "    batch = stack[i : i + BATCH_SIZE]\n",
    "    recoater_areas[i : i + BATCH_SIZE] = batch_predict_and_compute_areas(\n",
    "        finetuned, batch, [2], PIXEL_SIZE_MM2, imgsz=IMG_SIZE, conf=CONF_THRESH\n",
    "    )\n",
    "    spatter_areas[i : i + BATCH_SIZE] = batch_predict_and_compute_areas(\n",
    "        finetuned, batch, [1], PIXEL_SIZE_MM2, imgsz=IMG_SIZE, conf=CONF_THRESH\n",
    "    )\n",
    "    print(f\"Processed layers {i}–{i+BATCH_SIZE}\")\n",
    "\n",
    "# 4) assemble DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"layer\": np.arange(N),\n",
    "    \"recoater_mm2\": recoater_areas,\n",
    "    \"spatter_mm2\": spatter_areas,\n",
    "})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eeef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df.layer, df.recoater_mm2, label=\"Recoater\")\n",
    "plt.plot(df.layer, df.spatter_mm2, label=\"Spatter\")\n",
    "plt.xlabel(\"Layer\"); plt.ylabel(\"Anomaly Area (mm²)\")\n",
    "plt.legend(); plt.title(\"Per-Layer Anomaly Areas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5f650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1fa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e2775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8954d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d759bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_yolo_model(MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106f89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 296.5ms\n",
      "Speed: 3.8ms preprocess, 296.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 296.5ms\n",
      "Speed: 3.8ms preprocess, 296.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 295.4ms\n",
      "Speed: 3.5ms preprocess, 295.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 295.4ms\n",
      "Speed: 3.5ms preprocess, 295.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 clock, 277.5ms\n",
      "Speed: 3.3ms preprocess, 277.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 clock, 277.5ms\n",
      "Speed: 3.3ms preprocess, 277.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 278.9ms\n",
      "Speed: 4.5ms preprocess, 278.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 278.9ms\n",
      "Speed: 4.5ms preprocess, 278.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 265.9ms\n",
      "Speed: 3.4ms preprocess, 265.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 265.9ms\n",
      "Speed: 3.4ms preprocess, 265.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 323.2ms\n",
      "Speed: 4.2ms preprocess, 323.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 323.2ms\n",
      "Speed: 4.2ms preprocess, 323.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 267.2ms\n",
      "Speed: 3.8ms preprocess, 267.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 267.2ms\n",
      "Speed: 3.8ms preprocess, 267.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 274.6ms\n",
      "Speed: 3.6ms preprocess, 274.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 274.6ms\n",
      "Speed: 3.6ms preprocess, 274.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 clock, 320.1ms\n",
      "Speed: 6.1ms preprocess, 320.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 clock, 320.1ms\n",
      "Speed: 6.1ms preprocess, 320.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 319.3ms\n",
      "Speed: 4.6ms preprocess, 319.3ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 319.3ms\n",
      "Speed: 4.6ms preprocess, 319.3ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 2 tennis rackets, 1 clock, 270.3ms\n",
      "Speed: 3.4ms preprocess, 270.3ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 2 tennis rackets, 1 clock, 270.3ms\n",
      "Speed: 3.4ms preprocess, 270.3ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 285.4ms\n",
      "Speed: 3.6ms preprocess, 285.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 285.4ms\n",
      "Speed: 3.6ms preprocess, 285.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 276.7ms\n",
      "Speed: 5.0ms preprocess, 276.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 276.7ms\n",
      "Speed: 5.0ms preprocess, 276.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 258.8ms\n",
      "Speed: 3.4ms preprocess, 258.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 258.8ms\n",
      "Speed: 3.4ms preprocess, 258.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 274.1ms\n",
      "Speed: 3.5ms preprocess, 274.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 274.1ms\n",
      "Speed: 3.5ms preprocess, 274.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 clock, 280.1ms\n",
      "Speed: 3.4ms preprocess, 280.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 clock, 280.1ms\n",
      "Speed: 3.4ms preprocess, 280.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 285.5ms\n",
      "Speed: 5.3ms preprocess, 285.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 285.5ms\n",
      "Speed: 5.3ms preprocess, 285.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 291.6ms\n",
      "Speed: 3.8ms preprocess, 291.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 291.6ms\n",
      "Speed: 3.8ms preprocess, 291.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 278.5ms\n",
      "Speed: 3.6ms preprocess, 278.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 278.5ms\n",
      "Speed: 3.6ms preprocess, 278.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 452.5ms\n",
      "Speed: 9.1ms preprocess, 452.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 452.5ms\n",
      "Speed: 9.1ms preprocess, 452.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 449.5ms\n",
      "Speed: 6.4ms preprocess, 449.5ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 449.5ms\n",
      "Speed: 6.4ms preprocess, 449.5ms inference, 8.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 273.9ms\n",
      "Speed: 3.7ms preprocess, 273.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 273.9ms\n",
      "Speed: 3.7ms preprocess, 273.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 301.0ms\n",
      "Speed: 3.6ms preprocess, 301.0ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 301.0ms\n",
      "Speed: 3.6ms preprocess, 301.0ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 287.9ms\n",
      "Speed: 3.3ms preprocess, 287.9ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 287.9ms\n",
      "Speed: 3.3ms preprocess, 287.9ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 308.9ms\n",
      "Speed: 3.5ms preprocess, 308.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 308.9ms\n",
      "Speed: 3.5ms preprocess, 308.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 276.9ms\n",
      "Speed: 3.7ms preprocess, 276.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 276.9ms\n",
      "Speed: 3.7ms preprocess, 276.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 271.7ms\n",
      "Speed: 4.0ms preprocess, 271.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 271.7ms\n",
      "Speed: 4.0ms preprocess, 271.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 280.6ms\n",
      "Speed: 6.2ms preprocess, 280.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 280.6ms\n",
      "Speed: 6.2ms preprocess, 280.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 284.1ms\n",
      "Speed: 3.5ms preprocess, 284.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 284.1ms\n",
      "Speed: 3.5ms preprocess, 284.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 clock, 274.7ms\n",
      "Speed: 5.3ms preprocess, 274.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 clock, 274.7ms\n",
      "Speed: 5.3ms preprocess, 274.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 271.7ms\n",
      "Speed: 4.3ms preprocess, 271.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 271.7ms\n",
      "Speed: 4.3ms preprocess, 271.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 272.1ms\n",
      "Speed: 4.6ms preprocess, 272.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 272.1ms\n",
      "Speed: 4.6ms preprocess, 272.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 283.0ms\n",
      "Speed: 4.1ms preprocess, 283.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 283.0ms\n",
      "Speed: 4.1ms preprocess, 283.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 266.2ms\n",
      "Speed: 3.4ms preprocess, 266.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 266.2ms\n",
      "Speed: 3.4ms preprocess, 266.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 272.6ms\n",
      "Speed: 4.1ms preprocess, 272.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 272.6ms\n",
      "Speed: 4.1ms preprocess, 272.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 294.9ms\n",
      "Speed: 3.6ms preprocess, 294.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 294.9ms\n",
      "Speed: 3.6ms preprocess, 294.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 3 tennis rackets, 1 clock, 290.4ms\n",
      "Speed: 4.8ms preprocess, 290.4ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 3 tennis rackets, 1 clock, 290.4ms\n",
      "Speed: 4.8ms preprocess, 290.4ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 298.6ms\n",
      "Speed: 3.6ms preprocess, 298.6ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 298.6ms\n",
      "Speed: 3.6ms preprocess, 298.6ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 280.7ms\n",
      "Speed: 5.3ms preprocess, 280.7ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 280.7ms\n",
      "Speed: 5.3ms preprocess, 280.7ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 284.6ms\n",
      "Speed: 3.2ms preprocess, 284.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 284.6ms\n",
      "Speed: 3.2ms preprocess, 284.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 clock, 281.2ms\n",
      "Speed: 3.4ms preprocess, 281.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 clock, 281.2ms\n",
      "Speed: 3.4ms preprocess, 281.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 408.9ms\n",
      "Speed: 11.7ms preprocess, 408.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 408.9ms\n",
      "Speed: 11.7ms preprocess, 408.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 278.9ms\n",
      "Speed: 3.9ms preprocess, 278.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 278.9ms\n",
      "Speed: 3.9ms preprocess, 278.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 262.3ms\n",
      "Speed: 3.2ms preprocess, 262.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 262.3ms\n",
      "Speed: 3.2ms preprocess, 262.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 292.9ms\n",
      "Speed: 3.3ms preprocess, 292.9ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 292.9ms\n",
      "Speed: 3.3ms preprocess, 292.9ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 459.9ms\n",
      "Speed: 10.1ms preprocess, 459.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 459.9ms\n",
      "Speed: 10.1ms preprocess, 459.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 329.9ms\n",
      "Speed: 8.7ms preprocess, 329.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 329.9ms\n",
      "Speed: 8.7ms preprocess, 329.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 300.4ms\n",
      "Speed: 3.6ms preprocess, 300.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 300.4ms\n",
      "Speed: 3.6ms preprocess, 300.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 294.1ms\n",
      "Speed: 3.7ms preprocess, 294.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 294.1ms\n",
      "Speed: 3.7ms preprocess, 294.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 466.4ms\n",
      "Speed: 10.6ms preprocess, 466.4ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 466.4ms\n",
      "Speed: 10.6ms preprocess, 466.4ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 278.0ms\n",
      "Speed: 3.5ms preprocess, 278.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 278.0ms\n",
      "Speed: 3.5ms preprocess, 278.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 271.3ms\n",
      "Speed: 4.2ms preprocess, 271.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 271.3ms\n",
      "Speed: 4.2ms preprocess, 271.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 268.5ms\n",
      "Speed: 5.8ms preprocess, 268.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 268.5ms\n",
      "Speed: 5.8ms preprocess, 268.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 283.2ms\n",
      "Speed: 4.4ms preprocess, 283.2ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 283.2ms\n",
      "Speed: 4.4ms preprocess, 283.2ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 1 clock, 280.1ms\n",
      "Speed: 3.5ms preprocess, 280.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 1 clock, 280.1ms\n",
      "Speed: 3.5ms preprocess, 280.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 457.2ms\n",
      "Speed: 10.2ms preprocess, 457.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 457.2ms\n",
      "Speed: 10.2ms preprocess, 457.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 554.8ms\n",
      "Speed: 10.2ms preprocess, 554.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 554.8ms\n",
      "Speed: 10.2ms preprocess, 554.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 295.7ms\n",
      "Speed: 3.6ms preprocess, 295.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 295.7ms\n",
      "Speed: 3.6ms preprocess, 295.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 265.7ms\n",
      "Speed: 3.2ms preprocess, 265.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 265.7ms\n",
      "Speed: 3.2ms preprocess, 265.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 292.2ms\n",
      "Speed: 3.8ms preprocess, 292.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 292.2ms\n",
      "Speed: 3.8ms preprocess, 292.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 1 tennis racket, 269.6ms\n",
      "Speed: 3.5ms preprocess, 269.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 1 tennis racket, 269.6ms\n",
      "Speed: 3.5ms preprocess, 269.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 274.5ms\n",
      "Speed: 3.4ms preprocess, 274.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 274.5ms\n",
      "Speed: 3.4ms preprocess, 274.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n",
      "0: 640x640 (no detections), 289.6ms\n",
      "Speed: 4.7ms preprocess, 289.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "0: 640x640 (no detections), 289.6ms\n",
      "Speed: 4.7ms preprocess, 289.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n\u001b[32m      4\u001b[39m     img = load_hdf5_slice(DATA_PATH, layer, CAMERA_PATH)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     results = \u001b[43mrun_yolo_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     ar = compute_anomaly_area(\n\u001b[32m      7\u001b[39m         extract_anomaly_mask(results, RECOATER_CLASS, img.shape[:\u001b[32m2\u001b[39m]),\n\u001b[32m      8\u001b[39m         pixel_size_mm2=\u001b[32m0.01\u001b[39m)\n\u001b[32m      9\u001b[39m     sp = compute_anomaly_area(\n\u001b[32m     10\u001b[39m         extract_anomaly_mask(results, SPATTER_CLASS, img.shape[:\u001b[32m2\u001b[39m]),\n\u001b[32m     11\u001b[39m         pixel_size_mm2=\u001b[32m0.01\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/piml-in-metal-am/src/utils/yolo_segmentation.py:63\u001b[39m, in \u001b[36mrun_yolo_segmentation\u001b[39m\u001b[34m(model, image)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_yolo_segmentation\u001b[39m(model: Any, image: np.ndarray) -> Any:\n\u001b[32m     62\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run YOLO segmentation on an image.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/model.py:182\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    155\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    157\u001b[39m     **kwargs: Any,\n\u001b[32m    158\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/model.py:552\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/predictor.py:218\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/predictor.py:329\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/engine/predictor.py:173\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    168\u001b[39m visualize = (\n\u001b[32m    169\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    172\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:592\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/nn/tasks.py:115\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/nn/tasks.py:133\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/nn/tasks.py:154\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    155\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:91\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_layers =  len(h5py.File(DATA_PATH, \"r\")[CAMERA_PATH])\n",
    "areas = []\n",
    "for layer in range(n_layers):\n",
    "    img = load_hdf5_slice(DATA_PATH, layer, CAMERA_PATH)\n",
    "    results = run_yolo_segmentation(model, img)\n",
    "    ar = compute_anomaly_area(\n",
    "        extract_anomaly_mask(results, RECOATER_CLASS, img.shape[:2]),\n",
    "        pixel_size_mm2=0.01)\n",
    "    sp = compute_anomaly_area(\n",
    "        extract_anomaly_mask(results, SPATTER_CLASS, img.shape[:2]),\n",
    "        pixel_size_mm2=0.01)\n",
    "    areas.append({\"layer\": layer, \"recoater_mm2\": ar, \"spatter_mm2\": sp})\n",
    "\n",
    "df = pd.DataFrame(areas)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abcc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df.layer, df.recoater_mm2, label=\"Recoater Streaking\")\n",
    "plt.plot(df.layer, df.spatter_mm2, label=\"Spatter\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Anomaly Area (mm²)\")\n",
    "plt.legend()\n",
    "plt.title(\"Per-Layer Anomaly Areas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(IMG_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(LBL_TRAIN_DIR, exist_ok=True)\n",
    "\n",
    "with h5py.File(DATA_PATH, \"r\") as h5:\n",
    "    n_layers = h5[CAMERA_PATH].shape[0]\n",
    "    for layer in range(n_layers):\n",
    "        # save raw image\n",
    "        img = load_hdf5_slice(DATA_PATH, layer, CAMERA_PATH)\n",
    "        imageio.imwrite(f\"{IMG_TRAIN_DIR}/{layer:05d}.png\", img)\n",
    "        # combine masks\n",
    "        label = np.zeros(img.shape[:2], dtype=np.uint8)  # Background = 0\n",
    "        # Spatter (from HDF5 class 8) assigned to pixel value 1 (for YOLO class 0)\n",
    "        sp_mask = h5[\"slices/segmentation_results/8\"][layer].astype(bool)\n",
    "        label[sp_mask] = 1\n",
    "        # Streak (from HDF5 class 3) assigned to pixel value 2 (for YOLO class 1)\n",
    "        st_mask = h5[\"slices/segmentation_results/3\"][layer].astype(bool)\n",
    "        label[st_mask] = 2\n",
    "        imageio.imwrite(f\"{LBL_TRAIN_DIR}/{layer:05d}.png\", label)\n",
    "print(\"Export complete:\", n_layers, \"layers\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
