{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc755382",
   "metadata": {},
   "source": [
    "# Benchmarking ETL & Accelerators\n",
    "\n",
    "This notebook benchmarks:\n",
    "1. **Pre-aggregate once, cache forever** (parquet vs recompute)\n",
    "2. **Parallel summarization** using ProcessPoolExecutor\n",
    "3. **Dask** array summarization\n",
    "4. **DuckDB** querying Parquet vs pandas filtering\n",
    "5. **Packbits** compression of masks to bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cbdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    import dask.array as da\n",
    "    DASK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DASK_AVAILABLE = False\n",
    "try:\n",
    "    import duckdb\n",
    "    DUCKDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DUCKDB_AVAILABLE = False\n",
    "\n",
    "# Paths\n",
    "H5 = pathlib.Path(\"../data/2021-07-13 TCR Phase 1 Build 1.hdf5\")\n",
    "OUT_DIR = pathlib.Path(\"../data/precomputed\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "PARQUET = OUT_DIR / \"layer_summary.parquet\"\n",
    "\n",
    "CLS_STREAK, CLS_SPATTER = 3, 8\n",
    "EDGE_FRAC = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3877bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextlib.contextmanager\n",
    "def timed(label):\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    print(f\"{label}: {end-start:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadffab",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Pre-aggregate once, cache forever\n",
    "\n",
    "Define and time:\n",
    "- `summarise_layers_vec()` to build and write Parquet\n",
    "- reading from Parquet with `pd.read_parquet()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd35359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_layers_vec(batch=128):\n",
    "    with h5py.File(H5, \"r\") as h5:\n",
    "        seg       = h5[\"slices/segmentation_results\"]\n",
    "        ds        = seg[str(CLS_STREAK)]\n",
    "        nL, ny, nx = ds.shape\n",
    "        edge      = int(nx * EDGE_FRAC)\n",
    "        out       = np.zeros((nL, 4), np.int64)\n",
    "\n",
    "        for i in tqdm(range(0, nL, batch), desc=\"Vectorised batches\"):\n",
    "            sl   = slice(i, min(i+batch, nL))\n",
    "            st   = ds[sl]\n",
    "            out[sl,0] = st.sum(axis=(1,2))\n",
    "            out[sl,1] = st[:,:, :edge].sum(axis=(1,2))\n",
    "            out[sl,2] = st[:,:, -edge:].sum(axis=(1,2))\n",
    "            sp   = seg[str(CLS_SPATTER)][sl]\n",
    "            out[sl,3] = sp.sum(axis=(1,2))\n",
    "\n",
    "    df = pd.DataFrame(out,\n",
    "                      columns=[\"streak_px\",\"streak_left\",\"streak_right\",\"spatter_px\"])\n",
    "    df.to_parquet(PARQUET)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238f2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2e62f5e77c4749b9718f959970a7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorised batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised build parquet: 100.63 s\n",
      "Read Parquet: 0.02 s\n"
     ]
    }
   ],
   "source": [
    "# Benchmark\n",
    "if PARQUET.exists():\n",
    "    PARQUET.unlink()\n",
    "\n",
    "with timed(\"Vectorised build parquet\"):\n",
    "    df_build = summarise_layers_vec()\n",
    "\n",
    "with timed(\"Read Parquet\"):\n",
    "    df_read = pd.read_parquet(PARQUET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ae590",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- Have to seperate ETL from analysis: Run vectorised summary once.\n",
    "- Might have to guard analysis notebooks, e.g.:\n",
    "\n",
    "\t\t```python\n",
    "\t\tP = Path(\"../data/precomputed/layer_summary.parquet\")\n",
    "\t\tif P.exists():\n",
    "\t\t\tlayer_df = pd.read_parquet(P)\n",
    "\t\telse:\n",
    "\t\t\tlayer_df = summarise_layers_vec()\n",
    "\t\t```\n",
    "- Version Parquet with DVC, so we only recompute when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2392fe",
   "metadata": {},
   "source": [
    "## 2. Parallel summarization with small-batch ThreadPoolExecutor\n",
    "\n",
    "Split into small batches to cap per-thread memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3072a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 32\n",
    "with h5py.File(H5, \"r\") as h5:\n",
    "    total_layers = h5[\"slices/segmentation_results\"][str(CLS_STREAK)].shape[0]\n",
    "\n",
    "ranges = [\n",
    "    slice(i, min(i + batch, total_layers))\n",
    "    for i in range(0, total_layers, batch)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f81c",
   "metadata": {},
   "source": [
    "Chunk‐processing function (reopens file per chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee3945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_chunk(slc):\n",
    "    with h5py.File(H5, \"r\") as h5:\n",
    "        seg = h5[\"slices/segmentation_results\"]\n",
    "        st  = seg[str(CLS_STREAK)][slc]\n",
    "        sp  = seg[str(CLS_SPATTER)][slc]\n",
    "        edge = int(st.shape[2] * EDGE_FRAC)\n",
    "        out = np.zeros((slc.stop - slc.start, 4), dtype=np.int64)\n",
    "        out[:,0] = st.sum(axis=(1,2))\n",
    "        out[:,1] = st[:,:, :edge].sum(axis=(1,2))\n",
    "        out[:,2] = st[:,:, -edge:].sum(axis=(1,2))\n",
    "        out[:,3] = sp.sum(axis=(1,2))\n",
    "    return slc.start, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1b080",
   "metadata": {},
   "source": [
    "Run in parallel with threads + progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff40470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbdc38453074726b1f859da5184c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parallel chunks:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel summarise: 169.90 s\n"
     ]
    }
   ],
   "source": [
    "n_workers = os.cpu_count()  # Set n_workers to the number of CPU cores\n",
    "\n",
    "results = []\n",
    "with timed(\"Parallel summarise\"), ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    for start, out in tqdm(\n",
    "        executor.map(summarise_chunk, ranges),\n",
    "        total=len(ranges),\n",
    "        desc=\"Parallel chunks\"\n",
    "    ):\n",
    "        results.append((start, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5f0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: x[0])\n",
    "df_list = [\n",
    "    pd.DataFrame(\n",
    "        chunk,\n",
    "        columns=[\"streak_px\", \"streak_left\", \"streak_right\", \"spatter_px\"],\n",
    "        index=range(start, start + chunk.shape[0])\n",
    "    )\n",
    "    for start, chunk in results\n",
    "]\n",
    "df_par = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce33c0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dask array summarization\n",
    "\n",
    "If Dask is available, benchmark array computation using dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask sum streak_px: 41.84 s\n",
      "First 5 sums: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if DASK_AVAILABLE:\n",
    "    with h5py.File(H5, \"r\") as h5:\n",
    "        ds = h5[\"slices/segmentation_results\"][str(CLS_STREAK)]\n",
    "        darr = da.from_array(ds, chunks=(128, ds.shape[1], ds.shape[2]))\n",
    "        with timed(\"Dask sum streak_px\"):\n",
    "            streak_sum = darr.sum(axis=(1, 2)).compute()\n",
    "        print(\"First 5 sums:\", streak_sum[:5])\n",
    "else:\n",
    "    print(\"Dask not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c12cf",
   "metadata": {},
   "source": [
    "\n",
    "## 4. DuckDB querying Parquet vs pandas filtering\n",
    "\n",
    "Benchmark filter of layers where streak_px > median using DuckDB SQL vs pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6458ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB query: 0.09 s\n",
      "Pandas filter: 0.00 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "median_val = df_read['streak_px'].median()\n",
    "\n",
    "if DUCKDB_AVAILABLE:\n",
    "    with timed(\"DuckDB query\"):\n",
    "        con = duckdb.connect()\n",
    "        res = con.execute(f\"SELECT * FROM '{PARQUET}' WHERE streak_px > {median_val}\").fetchall()\n",
    "    with timed(\"Pandas filter\"):\n",
    "        df_filt = df_read[df_read['streak_px'] > median_val]\n",
    "else:\n",
    "    print(\"DuckDB not installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c5d2e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Packbits compression of a mask slice\n",
    "\n",
    "Demonstrate memory and time savings using `np.packbits` on a boolean mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d8e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.packbits: 0.00 s\n",
      "np.unpackbits: 0.00 s\n",
      "Original size: 3392964 Packed size: 424121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File(H5,\"r\") as h5:\n",
    "    mask = h5[\"slices/segmentation_results\"][str(CLS_STREAK)][0] > 0\n",
    "\n",
    "arr = mask.astype(bool)\n",
    "\n",
    "with timed(\"np.packbits\"):\n",
    "    packed = np.packbits(arr, axis=None)\n",
    "\n",
    "with timed(\"np.unpackbits\"):\n",
    "    unpacked = np.unpackbits(packed, count=arr.size).reshape(arr.shape)\n",
    "\n",
    "print(\"Original size:\", arr.nbytes, \"Packed size:\", packed.nbytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f47ac",
   "metadata": {},
   "source": [
    "## DVC Stage Integration\n",
    "To automate ETL with DVC:\n",
    "```bash\n",
    "dvc stage add -n build_layer_summary \\\n",
    "  -d data/2021-07-13\\ TCR\\ Phase\\ 1\\ Build\\ 1.hdf5 \\\n",
    "  -o data/precomputed/layer_summary.parquet \\\n",
    "  \"python notebooks/benchmark_etl_refined.ipynb\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
